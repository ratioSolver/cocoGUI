<template>
  <n-grid y-gap="12" :cols="1" class="chat-container" style="grid-template-rows: 1fr auto; padding: 12px;">
    <n-grid-item>
      <n-list :id="props.chat_id" style="max-height: calc(100vh - 190px);" ref="chat" :show-divider="false">
        <n-list-item v-for="msg in messages(props.item.values)" :key="msg.timestamp.getTime()"
          :align="msg.me ? 'right' : 'left'">
          <n-tag v-if="msg.text" :type="msg.me ? 'primary' : 'default'" round>
            {{ msg.text }}
          </n-tag>
        </n-list-item>
      </n-list>
    </n-grid-item>
    <n-grid-item>
      <n-input v-model:value="message" placeholder="Type a message..." style="width: 100%;" @keyup.enter="send_message">
        <template #suffix>
          <n-button @click="open_mic" :bordered="false" v-if="has_speech_recognition"
            :disabled="!props.item.type.dynamic_properties.has('open_mic') || speaking">
            <n-icon :component="recognizing ? Mic24Filled : Mic24Regular" />
          </n-button>
          <n-button @click="send_message" :bordered="false">
            <n-icon :component="Send24Regular" />
          </n-button>
        </template>
      </n-input>
    </n-grid-item>
  </n-grid>
</template>

<script setup lang="ts">
import { NGrid, NGridItem, NList, NListItem, NTag, NInput, NButton, NIcon } from 'naive-ui';
import { Send24Regular, Mic24Regular, Mic24Filled } from '@vicons/fluent';
import { taxonomy } from '@/taxonomy';
import { coco } from '@/coco';
import { ref, watch } from 'vue';

const range = ref<[number, number]>([new Date(Date.now() - 1000 * 60 * 60 * 24 * 7).getTime(), new Date().getTime()]);

const props = withDefaults(defineProps<{ item: taxonomy.Item; chat_id: string; lang: string; voice: string | null; }>(), { chat_id: 'chat', lang: 'en-US', voice: null });
if (!props.item.values.length)
  coco.KnowledgeBase.getInstance().load_data(props.item, range.value[0], range.value[1]);

const chat = ref(null)

const message = ref('');

const recognition = new ((window as any).SpeechRecognition || (window as any).webkitSpeechRecognition)();
const has_speech_recognition = ref(!!recognition);
const recognizing = ref(false);
let pending_recognition = false;

const synthesis = window.speechSynthesis;
const speaking = ref(false);
let voice: SpeechSynthesisVoice | null = null;
synthesis.onvoiceschanged = () => {
  for (const v of synthesis.getVoices())
    console.debug('Voice:', v.name, v.lang);
  console.debug('Desired voice:', props.voice);
  voice = props.voice ? synthesis.getVoices().find((v: SpeechSynthesisVoice) => v.name === props.voice)! : synthesis.getVoices().find((v: SpeechSynthesisVoice) => v.lang === props.lang)!;
  console.debug('Selected voice:', voice);
};

// Check if the browser supports the SpeechRecognition API
if (recognition) {
  recognition.continuous = false;
  recognition.interimResults = true;
  recognition.lang = props.lang;

  recognition.onresult = (event: any) => {
    message.value = event.results[0][0].transcript;
  };

  recognition.onspeechend = () => {
    close_mic();
    send_message();
  };

  recognition.onerror = (event: any) => {
    console.error('Speech recognition error:', event.error);
    close_mic();
  };

  watch(() => props.item.value, () => {
    if (props.item.value.data.open_mic && !recognizing.value) { // Open mic sent by the server
      if (!pending_recognition)
        open_mic();
      else { // We are speaking, so we enqueue the recognition
        console.debug('We enqueue the recognition');
        pending_recognition = true;
      }
    }
    else if (!props.item.value.data.open_mic && recognizing.value) { // Close mic sent by the server
      pending_recognition = false;
      close_mic();
    }
  });
}

const open_mic = () => {
  console.debug('Opening micrphone');
  const data: Record<string, any> = {};
  data.open_mic = true;
  coco.KnowledgeBase.getInstance().publish(props.item, data);
  recognizing.value = true;
  recognition.start();
};

const close_mic = () => {
  console.debug('Closing micrphone');
  const data: Record<string, any> = {};
  data.open_mic = false;
  coco.KnowledgeBase.getInstance().publish(props.item, data);
  recognizing.value = false;
  recognition.stop();
};

const syntesize = (text: string) => {
  console.debug('Synthesizing: ', text);
  const utterance = new SpeechSynthesisUtterance(text);
  if (voice)
    utterance.voice = voice;
  speaking.value = true;
  utterance.onerror = (event: SpeechSynthesisErrorEvent) => {
    console.error('Speech synthesis error:', event.error);
    speaking.value = false;
  };
  utterance.onend = () => {
    speaking.value = false;
    if (pending_recognition) {
      console.debug('We dequeue the recognition');
      open_mic();
      pending_recognition = false;
    }
  };
  synthesis.speak(utterance);
};

const send_message = () => {
  if (!message.value)
    return;

  const data: Record<string, any> = {};
  data.me = true;
  data.text = message.value;
  coco.KnowledgeBase.getInstance().publish(props.item, data);

  message.value = '';
};

watch(() => props.item.values, () => (chat.value! as HTMLDivElement).scrollTop = (chat.value! as HTMLDivElement).scrollHeight);
watch(() => props.item.value.data, (value => {
  if (value.text && !value.me)
    syntesize(value.text);
}));
</script>

<script lang="ts">
function messages(data: taxonomy.Data[]) {
  return data.filter((v: taxonomy.Data) => Object.keys(v.data).includes('text')).map((v: taxonomy.Data) => {
    return {
      timestamp: new Date(v.timestamp),
      me: v.data.me,
      text: v.data.text
    };
  });
}
</script>

<style scoped>
.chat-container {
  height: 100%;
  width: 100%;
}
</style>